{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stqcBP1Pb28z"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from skimage import color\n",
        "import os\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from torchvision.io import read_image\n",
        "from skimage.io import imread\n",
        "import threading\n",
        "import time\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QiViQ0FbdKC"
      },
      "source": [
        "# Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASZ3s5Vzaq3O"
      },
      "outputs": [],
      "source": [
        "def calc_deltaE(source, target, color_chart_area):\n",
        "  source = cv2.cvtColor(source, cv2.COLOR_BGR2RGB)\n",
        "  target = cv2.cvtColor(target, cv2.COLOR_BGR2RGB)\n",
        "  source = color.rgb2lab(source)\n",
        "  target = color.rgb2lab(target)\n",
        "  source = np.reshape(source, [-1, 3]).astype(np.float32)\n",
        "  target = np.reshape(target, [-1, 3]).astype(np.float32)\n",
        "  delta_e = np.sqrt(np.sum(np.power(source - target, 2), 1))\n",
        "  return sum(delta_e) / (np.shape(delta_e)[0] - color_chart_area)\n",
        "\n",
        "def calc_deltaE2000(source, target, color_chart_area):\n",
        "  source = cv2.cvtColor(source, cv2.COLOR_BGR2RGB)\n",
        "  target = cv2.cvtColor(target, cv2.COLOR_BGR2RGB)\n",
        "  source = color.rgb2lab(source)\n",
        "  target = color.rgb2lab(target)\n",
        "  source = np.reshape(source, [-1, 3]).astype(np.float32)\n",
        "  target = np.reshape(target, [-1, 3]).astype(np.float32)\n",
        "  deltaE00 = deltaE2000(source, target)\n",
        "  return sum(deltaE00) / (np.shape(deltaE00)[0] - color_chart_area)\n",
        "\n",
        "\n",
        "def deltaE2000(Labstd, Labsample):\n",
        "  kl = 1\n",
        "  kc = 1\n",
        "  kh = 1\n",
        "  Lstd = np.transpose(Labstd[:, 0])\n",
        "  astd = np.transpose(Labstd[:, 1])\n",
        "  bstd = np.transpose(Labstd[:, 2])\n",
        "  Cabstd = np.sqrt(np.power(astd, 2) + np.power(bstd, 2))\n",
        "  Lsample = np.transpose(Labsample[:, 0])\n",
        "  asample = np.transpose(Labsample[:, 1])\n",
        "  bsample = np.transpose(Labsample[:, 2])\n",
        "  Cabsample = np.sqrt(np.power(asample, 2) + np.power(bsample, 2))\n",
        "  Cabarithmean = (Cabstd + Cabsample) / 2\n",
        "  G = 0.5 * (1 - np.sqrt((np.power(Cabarithmean, 7)) / (np.power(\n",
        "    Cabarithmean, 7) + np.power(25, 7))))\n",
        "  apstd = (1 + G) * astd\n",
        "  apsample = (1 + G) * asample\n",
        "  Cpsample = np.sqrt(np.power(apsample, 2) + np.power(bsample, 2))\n",
        "  Cpstd = np.sqrt(np.power(apstd, 2) + np.power(bstd, 2))\n",
        "  Cpprod = (Cpsample * Cpstd)\n",
        "  zcidx = np.argwhere(Cpprod == 0)\n",
        "  hpstd = np.arctan2(bstd, apstd)\n",
        "  hpstd[np.argwhere((np.abs(apstd) + np.abs(bstd)) == 0)] = 0\n",
        "  hpsample = np.arctan2(bsample, apsample)\n",
        "  hpsample = hpsample + 2 * np.pi * (hpsample < 0)\n",
        "  hpsample[np.argwhere((np.abs(apsample) + np.abs(bsample)) == 0)] = 0\n",
        "  dL = (Lsample - Lstd)\n",
        "  dC = (Cpsample - Cpstd)\n",
        "  dhp = (hpsample - hpstd)\n",
        "  dhp = dhp - 2 * np.pi * (dhp > np.pi)\n",
        "  dhp = dhp + 2 * np.pi * (dhp < (-np.pi))\n",
        "  dhp[zcidx] = 0\n",
        "  dH = 2 * np.sqrt(Cpprod) * np.sin(dhp / 2)\n",
        "  Lp = (Lsample + Lstd) / 2\n",
        "  Cp = (Cpstd + Cpsample) / 2\n",
        "  hp = (hpstd + hpsample) / 2\n",
        "  hp = hp - (np.abs(hpstd - hpsample) > np.pi) * np.pi\n",
        "  hp = hp + (hp < 0) * 2 * np.pi\n",
        "  hp[zcidx] = hpsample[zcidx] + hpstd[zcidx]\n",
        "  Lpm502 = np.power((Lp - 50), 2)\n",
        "  Sl = 1 + 0.015 * Lpm502 / np.sqrt(20 + Lpm502)\n",
        "  Sc = 1 + 0.045 * Cp\n",
        "  T = 1 - 0.17 * np.cos(hp - np.pi / 6) + 0.24 * np.cos(2 * hp) + \\\n",
        "      0.32 * np.cos(3 * hp + np.pi / 30) \\\n",
        "      - 0.20 * np.cos(4 * hp - 63 * np.pi / 180)\n",
        "  Sh = 1 + 0.015 * Cp * T\n",
        "  delthetarad = (30 * np.pi / 180) * np.exp(\n",
        "    - np.power((180 / np.pi * hp - 275) / 25, 2))\n",
        "  Rc = 2 * np.sqrt((np.power(Cp, 7)) / (np.power(Cp, 7) + np.power(25, 7)))\n",
        "  RT = - np.sin(2 * delthetarad) * Rc\n",
        "  klSl = kl * Sl\n",
        "  kcSc = kc * Sc\n",
        "  khSh = kh * Sh\n",
        "  de00 = np.sqrt(np.power((dL / klSl), 2) + np.power((dC / kcSc), 2) +\n",
        "                 np.power((dH / khSh), 2) + RT * (dC / kcSc) * (dH / khSh))\n",
        "  return de00\n",
        "\n",
        "\n",
        "def calc_mae(source, target, color_chart_area):\n",
        "  source = np.reshape(source, [-1, 3]).astype(np.float32)\n",
        "  target = np.reshape(target, [-1, 3]).astype(np.float32)\n",
        "  source_norm = np.sqrt(np.sum(np.power(source, 2), 1))\n",
        "  target_norm = np.sqrt(np.sum(np.power(target, 2), 1))\n",
        "  norm = source_norm * target_norm\n",
        "  L = np.shape(norm)[0]\n",
        "  inds = norm != 0\n",
        "  angles = np.sum(source[inds, :] * target[inds, :], 1) / norm[inds]\n",
        "  angles[angles > 1] = 1\n",
        "  f = np.arccos(angles)\n",
        "  f[np.isnan(f)] = 0\n",
        "  f = f * 180 / np.pi\n",
        "  return sum(f) / (L - color_chart_area)\n",
        "\n",
        "\n",
        "\n",
        "def calc_mse(source, target, color_chart_area):\n",
        "  source = np.reshape(source, [-1, 1]).astype(np.float64)\n",
        "  target = np.reshape(target, [-1, 1]).astype(np.float64)\n",
        "  mse = sum(np.power((source - target), 2))\n",
        "  return mse / ((np.shape(source)[\n",
        "    0]) - color_chart_area)\n",
        "\n",
        "\n",
        "def evaluate_cc(corrected, gt, color_chart_area, opt=1):\n",
        "  \"\"\"\n",
        "    Color constancy (white-balance correction) evaluation of a given corrected\n",
        "    image.\n",
        "    :param corrected: corrected image\n",
        "    :param gt: ground-truth image\n",
        "    :param color_chart_area: If there is a color chart in the image, that is\n",
        "     masked out from both images, this variable represents the number of pixels\n",
        "     of the color chart.\n",
        "    :param opt: determines the required error metric(s) to be reported.\n",
        "         Options:\n",
        "           opt = 1 delta E 2000 (default).\n",
        "           opt = 2 delta E 2000 and mean squared error (MSE)\n",
        "           opt = 3 delta E 2000, MSE, and mean angular eror (MAE)\n",
        "           opt = 4 delta E 2000, MSE, MAE, and delta E 76\n",
        "    :return: error(s) between corrected and gt images\n",
        "    \"\"\"\n",
        "\n",
        "  if opt == 1:\n",
        "    return calc_deltaE2000(corrected, gt, color_chart_area)\n",
        "  elif opt == 2:\n",
        "    return calc_deltaE2000(corrected, gt, color_chart_area), calc_mse(\n",
        "      corrected, gt, color_chart_area)\n",
        "  elif opt == 3:\n",
        "    return calc_deltaE2000(corrected, gt, color_chart_area), calc_mse(\n",
        "      corrected, gt, color_chart_area), calc_mae(corrected, gt,\n",
        "                                                 color_chart_area)\n",
        "  elif opt == 4:\n",
        "    return calc_deltaE2000(corrected, gt, color_chart_area), calc_mse(\n",
        "      corrected, gt, color_chart_area), calc_mae(\n",
        "      corrected, gt, color_chart_area), calc_deltaE(corrected, gt,\n",
        "                                                    color_chart_area)\n",
        "  else:\n",
        "    raise Exception('Error in evaluate_cc function')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_metadata(fileName, set, metadata_baseDir=''):\n",
        "    \"\"\"\n",
        "    Gets metadata (e.g., ground-truth file name, chart coordinates and area).\n",
        "    :param fileName: input filename\n",
        "    :param set: which dataset?--options includes: 'RenderedWB_Set1',\n",
        "      'RenderedWB_Set2', 'Rendered_Cube+'\n",
        "    :param metadata_baseDir: metadata directory (required for Set1 only)\n",
        "    :return: metadata for a given image\n",
        "    evaluation_examples.py provides some examples of how to use it\n",
        "    \"\"\"\n",
        "\n",
        "    fname, file_extension = os.path.splitext(fileName)  # get file parts\n",
        "    name = os.path.basename(fname)  # get only filename without the directory\n",
        "\n",
        "    if set == 'RenderedWB_Set1': # Rendered WB dataset (Set1)\n",
        "        metadatafile_color = name + '_color.txt' # chart's colors info.\n",
        "        metadatafile_mask = name + '_mask.txt' # chart's coordinate info.\n",
        "        # get color info.\n",
        "        f = open(os.path.join(metadata_baseDir, metadatafile_color), 'r')\n",
        "        C = f.read()\n",
        "        colors = np.zeros((3, 24))  # color chart colors\n",
        "        temp = re.split(',|\\n', C)\n",
        "        # 3 x 24 colors in the color chart\n",
        "        colors = np.reshape(np.asfarray(temp[:-1], float), (24, 3)).transpose()\n",
        "        # get coordinate info\n",
        "        f = open(os.path.join(metadata_baseDir, metadatafile_mask), 'r')\n",
        "        C = f.read()\n",
        "        temp = re.split(',|\\n', C)\n",
        "        # take only the first 4 elements (i.e., the color chart coordinates)\n",
        "        temp = temp[0:4]\n",
        "        mask = np.asfarray(temp, float)  # color chart mask coordinates\n",
        "        # get ground-truth file name\n",
        "        seperator = '_'\n",
        "        temp = name.split(seperator)\n",
        "        gt_file = seperator.join(temp[:-2])\n",
        "        gt_file = gt_file + '_G_AS.png'\n",
        "        # compute mask area\n",
        "        mask_area = mask[2] * mask[3]\n",
        "        # final metadata\n",
        "        data = {\"gt_filename\": gt_file, \"cc_colors\": colors, \"cc_mask\": mask,\n",
        "                \"cc_mask_area\": mask_area}\n",
        "\n",
        "    elif set == 'RenderedWB_Set2': # Rendered WB dataset (Set2)\n",
        "        data = {\"gt_filename\": name + file_extension, \"cc_colors\": None,\n",
        "                \"cc_mask\": None, \"cc_mask_area\": 0}\n",
        "\n",
        "    elif set == 'Rendered_Cube+': # Rendered Cube+\n",
        "        # get ground-truth filename\n",
        "        temp = name.split('_')\n",
        "        gt_file = temp[0] + file_extension\n",
        "        mask_area = 58373  # calibration obj's area is fixed over all images\n",
        "        data = {\"gt_filename\": gt_file, \"cc_colors\": None, \"cc_mask\": None,\n",
        "                \"cc_mask_area\": mask_area}\n",
        "    else:\n",
        "        raise Exception(\n",
        "            \"Invalid value for set variable. \" +\n",
        "            \"Please use: 'RenderedWB_Set1', 'RenderedWB_Set2', 'Rendered_Cube+'\")\n",
        "\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nz4lbyHbn_p"
      },
      "source": [
        "# Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anvEflhHcwmB",
        "outputId": "fc53a236-8e63-4bf8-d26f-72d8f6914815"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUiPK9fBco1S"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/drive/MyDrive/CC/datasets/Set1_ground_truth_images_wo_CC.zip -d Set1_ground_truth_images_wo_CC\n",
        "!unzip -q /content/drive/MyDrive/CC/datasets/Set1_input_images_wo_CC_JPG.zip -d Set1_input_images_wo_CC_JPG\n",
        "!unzip -q /content/drive/MyDrive/CC/datasets/Set1_input_images_metadata.zip -d Set1_input_images_metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAaxGk-7bYjx"
      },
      "outputs": [],
      "source": [
        "CHECKPOINTS = '/content/drive/MyDrive/CC/checkpoints'\n",
        "IN_DIR = '/content/Set1_input_images_wo_CC_JPG'\n",
        "GT_DIR = '/content/Set1_ground_truth_images_wo_CC'\n",
        "FOLD_DIR = '/content/drive/MyDrive/CC/datasets/Set1_folds'\n",
        "METADATA_DIR = '/content/Set1_input_images_metadata'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBRXE12VdHQJ",
        "outputId": "a32fce88-56be-43fb-a53e-808499e751f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "attention_unet_20220918_021432_10.pth  a-unet_aug_20220918_104239_30.pth\n",
            "attention_unet_20220918_021432_15.pth  a-unet_aug_20220918_104239_35.pth\n",
            "attention_unet_20220918_021432_20.pth  a-unet_aug_20220918_104239_40.pth\n",
            "attention_unet_20220918_021432_25.pth  a-unet_aug_20220918_104239_45.pth\n",
            "attention_unet_20220918_021432_30.pth  a-unet_aug_20220918_104239_50.pth\n",
            "a-unet_aug_20220918_104239_100.pth     a-unet_aug_20220918_104239_55.pth\n",
            "a-unet_aug_20220918_104239_105.pth     a-unet_aug_20220918_104239_5.pth\n",
            "a-unet_aug_20220918_104239_10.pth      a-unet_aug_20220918_104239_60.pth\n",
            "a-unet_aug_20220918_104239_110.pth     a-unet_aug_20220918_104239_65.pth\n",
            "a-unet_aug_20220918_104239_115.pth     a-unet_aug_20220918_104239_70.pth\n",
            "a-unet_aug_20220918_104239_120.pth     a-unet_aug_20220918_104239_75.pth\n",
            "a-unet_aug_20220918_104239_125.pth     a-unet_aug_20220918_104239_80.pth\n",
            "a-unet_aug_20220918_104239_130.pth     a-unet_aug_20220918_104239_85.pth\n",
            "a-unet_aug_20220918_104239_135.pth     a-unet_aug_20220918_104239_90.pth\n",
            "a-unet_aug_20220918_104239_140.pth     a-unet_aug_20220918_104239_95.pth\n",
            "a-unet_aug_20220918_104239_145.pth     a-unet_aug_20220918_104239_done.pth\n",
            "a-unet_aug_20220918_104239_150.pth     sa-unet_20220917_155653_75.pth\n",
            "a-unet_aug_20220918_104239_15.pth      sa_unet_aug_20220918_021432_35.pth\n",
            "a-unet_aug_20220918_104239_20.pth      sa_unet_aug_20220918_021432_5.pth\n",
            "a-unet_aug_20220918_104239_25.pth\n"
          ]
        }
      ],
      "source": [
        "!ls $CHECKPOINTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-My6VKN9eEVz"
      },
      "source": [
        "# Deep learning model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hI71lUwEIGtI"
      },
      "source": [
        "## Define Attention U-Net model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b_yn1fPIJt0"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(ConvBlock, self).__init__()\n",
        "\n",
        "        # number of input channels is a number of filters in the previous layer\n",
        "        # number of output channels is a number of filters in the current layer\n",
        "        # \"same\" convolutions\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpConv(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(UpConv, self).__init__()\n",
        "\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.up(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    \"\"\"Attention block with learnable parameters\"\"\"\n",
        "\n",
        "    def __init__(self, F_g, F_l, n_coefficients):\n",
        "        \"\"\"\n",
        "        :param F_g: number of feature maps (channels) in previous layer\n",
        "        :param F_l: number of feature maps in corresponding encoder layer, transferred via skip connection\n",
        "        :param n_coefficients: number of learnable multi-dimensional attention coefficients\n",
        "        \"\"\"\n",
        "        super(AttentionBlock, self).__init__()\n",
        "\n",
        "        self.W_gate = nn.Sequential(\n",
        "            nn.Conv2d(F_g, n_coefficients, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(n_coefficients)\n",
        "        )\n",
        "\n",
        "        self.W_x = nn.Sequential(\n",
        "            nn.Conv2d(F_l, n_coefficients, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(n_coefficients)\n",
        "        )\n",
        "\n",
        "        self.psi = nn.Sequential(\n",
        "            nn.Conv2d(n_coefficients, 1, kernel_size=1, stride=1, padding=0, bias=True),\n",
        "            nn.BatchNorm2d(1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, gate, skip_connection):\n",
        "        \"\"\"\n",
        "        :param gate: gating signal from previous layer\n",
        "        :param skip_connection: activation from corresponding encoder layer\n",
        "        :return: output activations\n",
        "        \"\"\"\n",
        "        g1 = self.W_gate(gate)\n",
        "        x1 = self.W_x(skip_connection)\n",
        "        psi = self.relu(g1 + x1)\n",
        "        psi = self.psi(psi)\n",
        "        out = skip_connection * psi\n",
        "        return out\n",
        "\n",
        "\n",
        "class AttentionUNet(nn.Module):\n",
        "\n",
        "    def __init__(self, img_ch=3, output_ch=3):\n",
        "        super(AttentionUNet, self).__init__()\n",
        "\n",
        "        self.MaxPool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.Conv1 = ConvBlock(img_ch, 24)\n",
        "        self.Conv2 = ConvBlock(24, 48)\n",
        "        self.Conv3 = ConvBlock(48, 96)\n",
        "        self.Conv4 = ConvBlock(96, 192)\n",
        "        self.Conv5 = ConvBlock(192, 384)\n",
        "\n",
        "        self.Up5 = UpConv(384, 192)\n",
        "        self.Att5 = AttentionBlock(F_g=192, F_l=192, n_coefficients=96)\n",
        "        self.UpConv5 = ConvBlock(384, 192)\n",
        "\n",
        "        self.Up4 = UpConv(192, 96)\n",
        "        self.Att4 = AttentionBlock(F_g=96, F_l=96, n_coefficients=48)\n",
        "        self.UpConv4 = ConvBlock(192, 96)\n",
        "\n",
        "        self.Up3 = UpConv(96, 48)\n",
        "        self.Att3 = AttentionBlock(F_g=48, F_l=48, n_coefficients=24)\n",
        "        self.UpConv3 = ConvBlock(96, 48)\n",
        "\n",
        "        self.Up2 = UpConv(48, 24)\n",
        "        self.Att2 = AttentionBlock(F_g=24, F_l=24, n_coefficients=12)\n",
        "        self.UpConv2 = ConvBlock(48, 24)\n",
        "\n",
        "        self.Conv = nn.Conv2d(24, output_ch, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        e : encoder layers\n",
        "        d : decoder layers\n",
        "        s : skip-connections from encoder layers to decoder layers\n",
        "        \"\"\"\n",
        "        e1 = self.Conv1(x)\n",
        "\n",
        "        e2 = self.MaxPool(e1)\n",
        "        e2 = self.Conv2(e2)\n",
        "\n",
        "        e3 = self.MaxPool(e2)\n",
        "        e3 = self.Conv3(e3)\n",
        "\n",
        "        e4 = self.MaxPool(e3)\n",
        "        e4 = self.Conv4(e4)\n",
        "\n",
        "        e5 = self.MaxPool(e4)\n",
        "        e5 = self.Conv5(e5)\n",
        "\n",
        "        d5 = self.Up5(e5)\n",
        "\n",
        "        s4 = self.Att5(gate=d5, skip_connection=e4)\n",
        "        d5 = torch.cat((s4, d5), dim=1) # concatenate attention-weighted skip connection with previous layer output\n",
        "        d5 = self.UpConv5(d5)\n",
        "\n",
        "        d4 = self.Up4(d5)\n",
        "        s3 = self.Att4(gate=d4, skip_connection=e3)\n",
        "        d4 = torch.cat((s3, d4), dim=1)\n",
        "        d4 = self.UpConv4(d4)\n",
        "\n",
        "        d3 = self.Up3(d4)\n",
        "        s2 = self.Att3(gate=d3, skip_connection=e2)\n",
        "        d3 = torch.cat((s2, d3), dim=1)\n",
        "        d3 = self.UpConv3(d3)\n",
        "\n",
        "        d2 = self.Up2(d3)\n",
        "        s1 = self.Att2(gate=d2, skip_connection=e1)\n",
        "        d2 = torch.cat((s1, d2), dim=1)\n",
        "        d2 = self.UpConv2(d2)\n",
        "\n",
        "        out = self.Conv(d2)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a44NerBza6gm"
      },
      "source": [
        "## Simple Attention U-Net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfOW063mEgx5"
      },
      "outputs": [],
      "source": [
        "class SimpleAttentionUNet(nn.Module):\n",
        "    def __init__(self, img_ch=3, output_ch=3):\n",
        "        super(SimpleAttentionUNet, self).__init__()\n",
        "\n",
        "        self.MaxPool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.Conv1 = ConvBlock(img_ch, 24)\n",
        "        self.Conv2 = ConvBlock(24, 48)\n",
        "        self.Conv3 = ConvBlock(48, 96)\n",
        "        self.Conv4 = ConvBlock(96, 192)\n",
        "\n",
        "        self.Up4 = UpConv(192, 96)\n",
        "        self.Att4 = AttentionBlock(F_g=96, F_l=96, n_coefficients=48)\n",
        "        self.UpConv4 = ConvBlock(192, 96)\n",
        "\n",
        "        self.Up3 = UpConv(96, 48)\n",
        "        self.Att3 = AttentionBlock(F_g=48, F_l=48, n_coefficients=24)\n",
        "        self.UpConv3 = ConvBlock(96, 48)\n",
        "\n",
        "        self.Up2 = UpConv(48, 24)\n",
        "        self.Att2 = AttentionBlock(F_g=24, F_l=24, n_coefficients=12)\n",
        "        self.UpConv2 = ConvBlock(48, 24)\n",
        "\n",
        "        self.Conv = nn.Conv2d(24, output_ch, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        e : encoder layers\n",
        "        d : decoder layers\n",
        "        s : skip-connections from encoder layers to decoder layers\n",
        "        \"\"\"\n",
        "        e1 = self.Conv1(x)\n",
        "\n",
        "        e2 = self.MaxPool(e1)\n",
        "        e2 = self.Conv2(e2)\n",
        "\n",
        "        e3 = self.MaxPool(e2)\n",
        "        e3 = self.Conv3(e3)\n",
        "\n",
        "        e4 = self.MaxPool(e3)\n",
        "        e4 = self.Conv4(e4)\n",
        "\n",
        "        # e5 = self.MaxPool(e4)\n",
        "        # e5 = self.Conv5(e5)\n",
        "\n",
        "        # d5 = self.Up5(e5)\n",
        "\n",
        "        # s4 = self.Att5(gate=d5, skip_connection=e4)\n",
        "        # d5 = torch.cat((s4, d5), dim=1) # concatenate attention-weighted skip connection with previous layer output\n",
        "        # d5 = self.UpConv5(d5)\n",
        "\n",
        "        # d4 = self.Up4(d5)\n",
        "        d4 = self.Up4(e4)\n",
        "        s3 = self.Att4(gate=d4, skip_connection=e3)\n",
        "        d4 = torch.cat((s3, d4), dim=1)\n",
        "        d4 = self.UpConv4(d4)\n",
        "\n",
        "        d3 = self.Up3(d4)\n",
        "        s2 = self.Att3(gate=d3, skip_connection=e2)\n",
        "        d3 = torch.cat((s2, d3), dim=1)\n",
        "        d3 = self.UpConv3(d3)\n",
        "\n",
        "        d2 = self.Up2(d3)\n",
        "        s1 = self.Att2(gate=d2, skip_connection=e1)\n",
        "        d2 = torch.cat((s1, d2), dim=1)\n",
        "        d2 = self.UpConv2(d2)\n",
        "\n",
        "        out = self.Conv(d2)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkif8gdZghz6"
      },
      "source": [
        "# Inference model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DENaRa90W9h1"
      },
      "source": [
        "## Post process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tTx1rmhgkVp"
      },
      "outputs": [],
      "source": [
        "def kernelP(I):\n",
        "    \"\"\" Kernel function: kernel(r, g, b) -> (r,g,b,rg,rb,gb,r^2,g^2,b^2,rgb,1)\n",
        "        Ref: Hong, et al., \"A study of digital camera colorimetric characterization\n",
        "         based on polynomial modeling.\" Color Research & Application, 2001. \"\"\"\n",
        "    return (np.transpose((I[:, 0], I[:, 1], I[:, 2], I[:, 0] * I[:, 1], I[:, 0] * I[:, 2],\n",
        "                          I[:, 1] * I[:, 2], I[:, 0] * I[:, 0], I[:, 1] * I[:, 1],\n",
        "                          I[:, 2] * I[:, 2], I[:, 0] * I[:, 1] * I[:, 2],\n",
        "                          np.repeat(1, np.shape(I)[0]))))\n",
        "\n",
        "\n",
        "def get_mapping_func(image1, image2):\n",
        "    \"\"\" Computes the polynomial mapping \"\"\"\n",
        "    image1 = np.reshape(image1, [-1, 3])\n",
        "    image2 = np.reshape(image2, [-1, 3])\n",
        "    m = LinearRegression().fit(kernelP(image1), image2)\n",
        "    return m\n",
        "\n",
        "\n",
        "def apply_mapping_func(image, m):\n",
        "    \"\"\" Applies the polynomial mapping \"\"\"\n",
        "    sz = image.shape\n",
        "    image = np.reshape(image, [-1, 3])\n",
        "    result = m.predict(kernelP(image))\n",
        "    result = np.reshape(result, [sz[0], sz[1], sz[2]])\n",
        "    return result\n",
        "\n",
        "def outOfGamutClipping(I):\n",
        "    \"\"\" Clips out-of-gamut pixels. \"\"\"\n",
        "    I[I > 1] = 1  # any pixel is higher than 1, clip it to 1\n",
        "    I[I < 0] = 0  # any pixel is below 0, clip it to 0\n",
        "    return I"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTjUvwJlXGQ_"
      },
      "source": [
        "## DL model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20F2M9fYgnWB"
      },
      "outputs": [],
      "source": [
        "mytransform = transforms.Compose([\n",
        "    transforms.Resize((256, 256))\n",
        "])\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "model_checkpoint_path = os.path.join(CHECKPOINTS, 'a-unet_aug_20220918_104239_110.pth')\n",
        "# net = GridNet()\n",
        "net = AttentionUNet()\n",
        "# net = SimpleAttentionUNet()\n",
        "net.load_state_dict(torch.load(model_checkpoint_path))\n",
        "net.eval()\n",
        "net = net.to(device)\n",
        "\n",
        "\n",
        "def infer(image):\n",
        "    \"\"\"image: float np array RGB image\"\"\"\n",
        "    with torch.no_grad():\n",
        "        input = torch.from_numpy(image)\n",
        "        input = input.permute((2,0,1))\n",
        "        input = mytransform(input)\n",
        "        image1 = input.permute((1,2,0)).numpy()\n",
        "        input = input.to(torch.float32)\n",
        "        input = input.unsqueeze(0)\n",
        "        output = net(input.to(device))[0]\n",
        "    output = output.permute((1,2,0)).cpu().numpy()\n",
        "    mapping_func = get_mapping_func(image1, output)\n",
        "    return outOfGamutClipping(apply_mapping_func(image, mapping_func))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LA1EyG-ZfFnR"
      },
      "source": [
        "# Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbeP9SeHdqfi"
      },
      "outputs": [],
      "source": [
        "def eval(name, imgin, gtimg):\n",
        "    # read the image\n",
        "    I_in = cv2.imread(imgin, cv2.IMREAD_COLOR)\n",
        "    # read gt image\n",
        "    gt = cv2.imread(gtimg, cv2.IMREAD_COLOR)\n",
        "    # metadata\n",
        "    metadata = get_metadata(name, 'RenderedWB_Set1', METADATA_DIR)  \n",
        "\n",
        "    # white balance I_in\n",
        "    I_corr = (infer(I_in[:,:,::-1] / 255)[:,:,::-1]*255).astype(\"uint8\")  \n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(20,10))\n",
        "    plt.subplot(1,3,1)\n",
        "    plt.imshow(I_in[:,:,::-1])\n",
        "    plt.title(\"Input\")\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.imshow(gt[:,:,::-1])\n",
        "    plt.title(\"Target\")\n",
        "    plt.subplot(1,3,3)\n",
        "    plt.imshow(I_corr[:,:,::-1])\n",
        "    plt.title(\"Corrected\")\n",
        "    plt.show()\n",
        "\n",
        "    # Evaluation\n",
        "    deltaE00, MSE, MAE, deltaE76 = evaluate_cc(I_corr, gt, metadata[\"cc_mask_area\"],\n",
        "                                            opt=4)\n",
        "    # logger.info('DeltaE 2000: %0.2f, MSE= %0.2f, MAE= %0.2f, DeltaE 76= %0.2f\\n'\n",
        "    #     % (deltaE00, MSE, MAE, deltaE76))\n",
        "\n",
        "    return deltaE00, MSE.item(), MAE, deltaE76"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4VPOvxfCfPj"
      },
      "outputs": [],
      "source": [
        "class EvalThread(threading.Thread):\n",
        "  eval_log = pd.DataFrame(columns=[\"img_in\", \"img_gt\", \"deltaE2000\", \"MSE\", \"MAE\", \"deltaE76\"])\n",
        "\n",
        "  def __init__(self, part, n_part, imgs):\n",
        "    super().__init__()\n",
        "    self.part = part\n",
        "    self.n_part = n_part\n",
        "    self.imgs = imgs\n",
        "\n",
        "  def run(self):\n",
        "    imgs = self.imgs\n",
        "    df = EvalThread.eval_log\n",
        "    for i in range(len(imgs)):\n",
        "      if i % self.n_part == self.part:\n",
        "        img_in = os.path.join(IN_DIR, imgs[i] + '.jpg')\n",
        "        img_gt = os.path.join(GT_DIR, imgs[i].rsplit('_', maxsplit=2)[0] + '_G_AS.png')\n",
        "        deltaE00, MSE, MAE, deltaE76 = eval(imgs[i], img_in, img_gt)\n",
        "        df.loc[len(df)] = {\n",
        "            \"img_in\": img_in, \n",
        "            \"img_gt\": img_gt,\n",
        "            \"deltaE2000\": deltaE00,\n",
        "            \"MSE\": MSE,\n",
        "            \"MAE\": MAE,\n",
        "            \"deltaE76\": deltaE76\n",
        "        }\n",
        "        print(f'[AVG {len(df)}/{len(imgs)}] DeltaE 2000: {df[\"deltaE2000\"].mean():.2f}, MSE={df[\"MSE\"].mean():.2f}, MAE={df[\"MAE\"].mean():.2f}, DeltaE 76={df[\"deltaE76\"].mean():.2f}')\n",
        "      # if len(df) % 10 == 9:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ko3i7QFECt8K",
        "outputId": "3bb72b51-569a-46b0-f8d4-cdc2d51b44d6"
      },
      "outputs": [],
      "source": [
        "imgs = []    \n",
        "with open(os.path.join(FOLD_DIR, 'fold_1.txt'), 'r') as f:\n",
        "    imgs += [s.strip().split(\".\")[0] for s in f.readlines()]\n",
        "\n",
        "n_part = 8\n",
        "threads = []\n",
        "for part in range(n_part):\n",
        "  thread = EvalThread(part, n_part, imgs[:100])\n",
        "  threads.append(thread)\n",
        "\n",
        "for thread in threads:\n",
        "  thread.start()\n",
        "  thread.join()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [
        "hI71lUwEIGtI",
        "o-136eJ5Eo3r",
        "wkif8gdZghz6"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "59bc8e2c397f0f3dd4c3f46a78ceb843605bec637a61d7dfe6aa303fd84548fe"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
